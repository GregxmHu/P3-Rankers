{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenMatch.models import t5\n",
    "from transformers import T5Tokenizer\n",
    "import torch\n",
    "from OpenMatch.data import DataLoader\n",
    "from OpenMatch.data.datasets import t5Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=t5Dataset(\"/home/huxiaomeng/50-q-1-n_msmarco_train.jsonl\",tokenizer,template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/huxiaomeng/t5v11large were not used when initializing T5ForConditionalGeneration: ['encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.12.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.13.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.14.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.15.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.16.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.17.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.18.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.19.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.20.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.21.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.22.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.23.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at /home/huxiaomeng/t5v11large and are newly initialized: ['encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight', 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight', 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.16.layer.1.DenseReluDense.wi.weight', 'encoder.block.17.layer.1.DenseReluDense.wi.weight', 'encoder.block.18.layer.1.DenseReluDense.wi.weight', 'encoder.block.19.layer.1.DenseReluDense.wi.weight', 'encoder.block.20.layer.1.DenseReluDense.wi.weight', 'encoder.block.21.layer.1.DenseReluDense.wi.weight', 'encoder.block.22.layer.1.DenseReluDense.wi.weight', 'encoder.block.23.layer.1.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1],\n",
      "        [6136,    1]])\n"
     ]
    }
   ],
   "source": [
    "model=t5(\"/home/huxiaomeng/t5v11large\",True,False,[1,2,3],[4,5,6],[7,8,9]).to(\"cuda:6\")\n",
    "ckpt=\"/data/private/huxiaomeng/promptir/checkpoints/test_v11-large/q1000-n-1/_step-100.bin\"\n",
    "device=torch.device(\"cuda:6\")\n",
    "model.load_state_dict(torch.load(ckpt,map_location=device))\n",
    "tokenizer=T5Tokenizer.from_pretrained(\"/home/huxiaomeng/t5v11large/\")\n",
    "template=\"Query: <q> Document: <d> Relevant: \"\n",
    "dataset=t5Dataset(\"/home/huxiaomeng/new/maeningless.jsonl\",tokenizer,template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=10,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,test_batch  in enumerate(loader):\n",
    "    batch_score,_=model(\n",
    "                        input_ids=test_batch['input_ids'].to(device), \n",
    "                        attention_mask=test_batch['attention_mask'].to(device),\n",
    "                        query_ids=test_batch['query_ids'].to(device), \n",
    "                        doc_ids=test_batch['doc_ids'].to(device),\n",
    "                        query_attention_mask=test_batch['query_attention_mask'].to(device),\n",
    "                        doc_attention_mask=test_batch['doc_attention_mask'].to(device),\n",
    "                        labels=test_batch['labels'].to(device)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.6281e-22, 1.0000e+00, 4.9667e-22, 1.0000e+00, 6.6630e-22, 1.0000e+00,\n",
       "        5.6396e-22, 1.0000e+00, 3.3981e-22, 1.0000e+00], device='cuda:6',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_score.softmax(dim=-1)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.4811e-32, 4.7977e-32, 1.2697e-33, 1.0725e-34, 4.2548e-33, 1.6771e-34,\n",
       "        5.2594e-31, 2.7186e-33, 3.7752e-33, 5.2866e-34], device='cuda:6',\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " batch_score.softmax(dim=-1)[:, 1].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Stack(\n",
       "  (embed_tokens): Embedding(32128, 1024)\n",
       "  (block): ModuleList(\n",
       "    (0): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 16)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 16)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (21): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): T5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): T5LayerSelfAttention(\n",
       "          (SelfAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): T5LayerCrossAttention(\n",
       "          (EncDecAttention): T5Attention(\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): T5LayerFF(\n",
       "          (DenseReluDense): T5DenseReluDense(\n",
       "            (wi): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "            (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c2d794e841d63af5b0a8bf3b4ad4e950d3f51d9cfc7002dfe86825f379a2898"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('lmbff': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
